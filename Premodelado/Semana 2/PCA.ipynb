{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ip38LhwnKDrCjFTNgPFacQ.jpeg\" width=\"350px\" height=\"180px\" />\n",
    "\n",
    "\n",
    "# <font color= #8A0829> Laboratorio de Modelado de Datos </font>\n",
    "#### <font color= #2E9AFE> `Martes y Viernes (Videoconferencia) de 13:00 - 15:00 hrs`</font>\n",
    "- <Strong> Sara Eugenia Rodríguez </Strong>\n",
    "- <Strong> Año </Strong>: 2025\n",
    "- <Strong> Email: </Strong>  <font color=\"blue\"> `cd682324@iteso.mx` </font>\n",
    "___\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ip38LhwnKDrCjFTNgPFacQ.jpeg</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color= #2E9AFE> Tema: Reducción de dimensionalidad</font>\n",
    "\n",
    "La **maldición de la dimensionalidad** se refiere a los problemas que ocurren cuando los datos tienen demasiadas características o dimensiones.\n",
    "\n",
    "A medida que aumenta el número de dimensiones:\n",
    "\n",
    "- Los datos se vuelven dispersos.\n",
    "- Se vuelve más difícil encontrar patrones o relaciones a medida que se agrega más y más ruido a los datos.\n",
    "- Los algoritmos se vuelven más lentos y menos precisos.\n",
    "\n",
    "**Ejemplo:**\n",
    "\n",
    "Digamos que queremos predecir el precio de una casa. Al inicio tenemos tres variables:\n",
    "1. Tamaño de la casa\n",
    "2. Número de cuartos\n",
    "3. Ubicación\n",
    "\n",
    "Con esto es fácil encontrar patrones y hacer buenas predicciones. \n",
    "\n",
    "Podríamos empezar a pensar en más caracteríasticas que pudieran ayudarnos a ser más precisos en nuestras predicciones, como:\n",
    "\n",
    "- Color de las paredes\n",
    "- Tipo de cortinas\n",
    "- Forma de las agarraderas para abrir las puertas\n",
    "- Marca del fregadero de la cocina\n",
    "- Tipo de piso\n",
    "- Tipos de focos\n",
    "- Etc...\n",
    "\n",
    "Ahora hemos agregado más de 30 características... Por sentido común, la mayoría de estos realmente no afectan el precio, como la marca del fregadero de la cocina, el tipo de cortinas, etc.\n",
    "\n",
    "Entonces, no siempre es necesario que agregando más características siempre obtengamos mejores resultados.\n",
    "\n",
    "**Qué pasa cuando más características son agregadas?**\n",
    "\n",
    "- Es más difícil encontrar patrones que valgan la pena\n",
    "- El modelo se hace más lento y puede dar peores resultados\n",
    "- Necesitamos muchísima más data para entrenar el modelo\n",
    "\n",
    "### Esto es la maldición de la dimensionalidad: demasidas características pueden perjudicar nuestros resultados\n",
    "\n",
    "**Formas de quitar la maldición de la dimensionalidad**\n",
    "\n",
    "1. Selección de Variables\n",
    "2. Reducción de Dimensionalidad = Extracción de variables\n",
    "\n",
    "La selección de características es diferente a la reducción de dimensionalidad. \n",
    "\n",
    "Ambos métodos buscan reducir la cantidad de atributos en el conjunto de datos, pero un método de reducción de dimensionalidad lo hace creando nuevas combinaciones de atributos, donde los métodos de selección de características incluyen y excluyen atributos presentes en los datos sin cambiarlos.\n",
    "\n",
    "Ejemplos de métodos de reducción de dimensionalidad incluyen: Análisis de componentes principales y Descomposición de valores singulares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principant Component Analysis \n",
    "\n",
    "El PCA usa álgebra lineal para transformar el conjunto de datos en una forma comprimida.\n",
    "\n",
    "**El objetivo de PCA**\n",
    "\n",
    "Identificar patrones, correlaciones entre variables y luego comprimir las variables en sus características más importantes para que los datos se simplifiquen sin perder información importante.\n",
    "\n",
    "**Cómo determinar si el PCA es el mejor método para ti**\n",
    "\n",
    "1. ¿Quieres reducir el número de variables, pero no puedes eliminar ninguna por completo?\n",
    "2. Tienes demasiadas variables correlacionadas?\n",
    "3. Tus datos son lineales\n",
    "4. ¿Te sientes cómodo perdiendo algo de interpretabilidad de tus variables independientes?\n",
    "  \n",
    "Si respondió “sí” a las tres preguntas... entonces PCA es para ti.\n",
    "\n",
    "Por cierto...el PCA se beneficia de la estandarización de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerías\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargar datos\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='target')\n",
    "df = pd.concat([X,y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de datos\n",
    "\n",
    "Antes de realizar PCA, es fundamental estandarizar los datos. Esto significa centrar los datos restando la media y escalarlos dividiéndolos por la desviación estándar. La estandarización garantiza que todas las características tengan la misma importancia en el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escalar datos \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_scaled = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eligiendo el número de componentes principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    " \n",
    "#inicializar el objeto\n",
    "pca = PCA()\n",
    "#aplicarlo a los datos\n",
    "pca_fit = pca.fit_transform(x_scaled)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables transformadas (rotadas)\n",
    "pd.DataFrame(pca_fit, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar la varianza acumulativa de cada componente\n",
    "plt.figure(figsize = (15, 6))\n",
    "components = np.arange(1, 31, step=1) \n",
    "variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.ylim(0.0,1.1)\n",
    "plt.plot(components, variance, marker='o', linestyle='--', color='green')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(1, 14, step=1))\n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('The number of components needed to explain variance')\n",
    "plt.axhline(y=0.90, color='r', linestyle='-')\n",
    "plt.text(0.5, 0.85, '90% variance threshold', color = 'red', fontsize=16)\n",
    "plt.text(25, 0.85, \"Components needed: \"+str(np.where(np.cumsum(pca.explained_variance_ratio_)>=0.9)[0][0]), color = \"red\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gráfico anterior contiene la varianza acumulada como una proporción en el eje Y y el número de componentes en el eje X.\n",
    "\n",
    "Utilizando un umbral de variación del 90%, el gráfico anterior nos ayuda a determinar cuántos componentes debemos conservar de nuestro conjunto de datos para que siga teniendo sentido para nosotros en cualquier modelado posterior.\n",
    "\n",
    "Tenga en cuenta que aquí elegimos el 90% como umbral de variación, pero esta no es una regla de oro. El data scientist elige este umbral de variación.\n",
    "\n",
    "Entonces, podemos ver en este gráfico que necesitamos 6 componentes para retener el 90% de la variabilidad (información) en nuestros datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando el PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    " \n",
    "#inicializar el objeto\n",
    "pca = PCA(n_components=6)\n",
    "#aplicarlo a los datos\n",
    "pca_fit = pca.fit_transform(x_scaled)\n",
    "datos_pca = pd.DataFrame(pca_fit, columns=['PC1','PC2','PC3','PC4','PC5','PC6'])\n",
    "datos_pca['y'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sin PCA\n",
    "##Aplicacion de la regresion logística con datos originales\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#inicializar el objeto\n",
    "logreg = LogisticRegression()\n",
    "#aplicar la regresión logística a los datos\n",
    "logreg.fit(x_scaled, y)\n",
    "\n",
    "#predicciones\n",
    "y_predict = logreg.predict(x_scaled)\n",
    "accuracy_score(y,y_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Aplicacion de la regresion logística con datos después de aplicar el PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(np.array(datos_pca[['PC1','PC2','PC3','PC4','PC5','PC6']]), y)\n",
    "\n",
    "y_predict = logreg.predict(datos_pca[['PC1','PC2','PC3','PC4','PC5','PC6']])\n",
    "accuracy_score(y,y_predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se aumenta el accuracy con el PCA (fenómeno normal) pero sí reducimos la dimensionalidad de los datos, por lo tanto es menos carga computacional. Con menos información, el clasificador sigue dando muy buenos resultados. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué algoritmos se benefician del PCA?\n",
    "\n",
    "1. Algoritmos lineales. Ya que ayuda a reducir la multicolinealidad\n",
    "    - Regresión logística\n",
    "    - Regresión Lineal\n",
    "2. Máquinas de vector soporte\n",
    "3. Algoritmos de clustering\n",
    "4. Redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más información sobre el PCA:\n",
    "\n",
    "https://towardsdatascience.com/the-most-gentle-introduction-to-principal-component-analysis-9ffae371e93b#:~:text=From%20Wikipedia%2C%20PCA%20is%20a,find%20unmeasured%20%E2%80%9Clatent%20variables%E2%80%9D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Sara E. Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
